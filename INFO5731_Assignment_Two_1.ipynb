{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Supraja1419/Supraja_INFO5731_Spring2022/blob/main/INFO5731_Assignment_Two_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 100 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.imdb.com/title/tt9376612/?ref_=vp_vi_tt\"\n",
        "\n",
        "r = requests.get(url=url)\n",
        "# create a BeautifulSoup object\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "\n",
        "    \n",
        "def getMovieDetails(url):\n",
        "    data = {}\n",
        "    r = requests.get(url=url)\n",
        "    # Create a BeautifulSoup object\n",
        "    soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "    #page title\n",
        "    title = soup.find('title')\n",
        "    data[\"title\"] = title.string\n",
        "\n",
        "    # rating\n",
        "    ratingValue = soup.find(\"span\", {\"itemprop\" : \"ratingValue\"})\n",
        "    data[\"ratingValue\"] = ratingValue.string\n",
        "\n",
        "    # no of rating given\n",
        "    ratingCount = soup.find(\"span\", {\"itemprop\" : \"ratingCount\"})\n",
        "    data[\"ratingCount\"] = ratingCount.string\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "outputId": "cd8fa5e6-5925-4ed3-f468-baa3425dec37"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (Temp/ipykernel_27320/1239294766.py, line 11)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Supraja\\AppData\\Local\\Temp/ipykernel_27320/1239294766.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    from nitk.tokenize import word tokenize\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# load text\n",
        "# Write your code here\n",
        "\n",
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nItk.corpus\n",
        "from nitk.corpus import stopwords\n",
        "from nItk.stem import PorterStemmer\n",
        "from nitk.tokenize import word tokenize\n",
        "from nItk.stem import WordNetLemmatizer\n",
        "# Get all stopwords\n",
        "stpwordlist = stopwords.words('english')\n",
        "# Read the dataset\n",
        "df = pd.read _csv(\"data.csv\")\n",
        "#lemmatization function\n",
        "def lemmatization(string):\n",
        "txt = [WordNetLemmatizer().lemmatize(x) for x in string]\n",
        "return txt\n",
        "#stemming function\n",
        "def stemming (string):\n",
        "txt = [PorterStemmer().stem(x) for x in string]\n",
        "return txt\n",
        "# main function\n",
        "def clean text(dataframe, tt):\n",
        "# Remove the punctuations and noise\n",
        "dataframe[tt]=dataframe[tt].apply(lambda string: re.sub(r\" (@[A-Za-z0-9]+)|([^0-9A-Za-z\n",
        "It1)1@w+:WS+)|^rtIhtto.+?\" '''' string))\n",
        "# Remove the numbers\n",
        "dataframe[tt]=dataframe[tt].apply(lambda string: re.sub(r\"\\d+\", '\", string))\n",
        "# Remove the stopwords\n",
        "dataframe[tt] = dataframe[tt].apply(lambda string: \"'join( [w for w in string.split() if w not in(stpwordlist)]))\"\"\n",
        "# tokenize\n",
        "dataframe['toks'l=\n",
        "dataframe ['toks'].apply (lambda x:word tokenize(x))\n",
        "# Convert to lower string\n",
        "dataframe[tt]=dataframe[tt].str.lower()\n",
        "# Stemming and Lemmatization\n",
        "dataframe['toks'].apply(lambda string:word_stemmer(string))\n",
        "dataframe['toks'].apply(lambda string:word_lemmatizer(string))\n",
        "return dataframe\n",
        "clean text (df \"textColumn\")\n",
        "Done\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "outputId": "e20c18c2-0622-4417-b10b-a17d171801bb"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'selenium'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27320/1758893622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mison\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#define the url to extract reviews\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "# import all the necessary modules\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "import ison\n",
        "#define the url to extract reviews\n",
        "url='https://www.imdb.com/title/tt7286456/reviews'\n",
        "sort='userRating&dir=desc&ratingFilter=0'\n",
        "#open the url in a new chrome window\n",
        "driver = webdriver.Chrome(executable_path='C:|webdriversdriver.get(url)')\n",
        "#load all the pages\n",
        "while True:\n",
        "    try:\n",
        "        driver.find_element_by_css_selector('#load-more-cc-objects').click()\n",
        "    except:\n",
        "            break\n",
        "#get a response and search for reviews\n",
        "soup = BeautifulSoup(driver.page_source,features=\"html.parser\")\n",
        "reviews = soup.findAll ('div', attrs={'class': 'textshow-more control'})\n",
        "#get individual review in a list\n",
        "listOfReviews=[review.get_text().replace('In', \"''\") for review in reviews]\n",
        "#make a dataframe from list\n",
        "df = pd.DataFrame({'User Reviews':listOfReviews})\n",
        "#save the dataframe on a cv file\n",
        "df.to_csv('userReviews.csv')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2zwVjL1vzzn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "INFO5731_Assignment_Two-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}